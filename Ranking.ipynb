{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oOejWJD-XoPl"
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "path = \".\"\n",
    "filename = \"crowd_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TirgjX2zXoPp"
   },
   "outputs": [],
   "source": [
    "with open(\"%s/%s\"%(path,filename),\"rb\") as _in:\n",
    "    similarity_dataset = pkl.load(_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsubJI7vXoPs"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author's Note:\n",
    "This code aligns the textual description with the video clips. In order to get the textual\n",
    "  description corresponding to key #5 in the similarity dataset, simply call phrase_lookup[5]\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd # Pandas library enables data manipulation\n",
    "data_url = \"revisedBDDX.csv\"\n",
    "def load_bddx_data(csv_name):\n",
    "    column_names = ['Index', 'InputVideo', '1S', '1E', '1A', '1J', '2S', '2E', '2A', '2J', '3S', '3E', '3A', '3J',\n",
    "                    '4S', '4E', '4A', '4J','5S', '5E', '5A', '5J','6S', '6E', '6A', '6J','7S', '7E', '7A', '7J',\n",
    "                    '8S', '8E', '8A', '8J','9S', '9E', '9A', '9J','10S', '10E', '10A', '10J','11S', '11E', '11A', '11J',\n",
    "                    '12S', '12E', '12A', '12J','13S', '13E', '13A', '13J','14S', '14E', '14A', '14J','15S', '15E', '15A', '15J']\n",
    "    \n",
    "    return pd.read_csv(csv_name, names=column_names)\n",
    "\n",
    "bddx = load_bddx_data(data_url)\n",
    "bddx = bddx.drop(['1S', '1E','2S', '2E','3S', '3E','4S', '4E','5S', '5E','6S', '6E','7S', '7E','8S', '8E','9S', '9E','10S', '10E','11S', '11E','12S', '12E','13S', '13E','14S', '14E','15S', '15E', ], axis=1)\n",
    "bddx = bddx.fillna(\"\")\n",
    "\n",
    "bddx['1AJ'] = bddx[['1A', '1J']].agg(' '.join, axis=1)\n",
    "bddx['2AJ'] = bddx[['2A', '2J']].agg(' '.join, axis=1)\n",
    "bddx['3AJ'] = bddx[['3A', '3J']].agg(' '.join, axis=1)\n",
    "bddx['4AJ'] = bddx[['4A', '4J']].agg(' '.join, axis=1)\n",
    "bddx['5AJ'] = bddx[['5A', '5J']].agg(' '.join, axis=1)\n",
    "bddx['6AJ'] = bddx[['6A', '6J']].agg(' '.join, axis=1)\n",
    "bddx['7AJ'] = bddx[['7A', '7J']].agg(' '.join, axis=1)\n",
    "bddx['8AJ'] = bddx[['8A', '8J']].agg(' '.join, axis=1)\n",
    "bddx['9AJ'] = bddx[['9A', '9J']].agg(' '.join, axis=1)\n",
    "bddx['10AJ'] = bddx[['10A', '10J']].agg(' '.join, axis=1)\n",
    "bddx['11AJ'] = bddx[['11A', '11J']].agg(' '.join, axis=1)\n",
    "bddx['12AJ'] = bddx[['12A', '12J']].agg(' '.join, axis=1)\n",
    "bddx['13AJ'] = bddx[['13A', '13J']].agg(' '.join, axis=1)\n",
    "bddx['14AJ'] = bddx[['14A', '14J']].agg(' '.join, axis=1)\n",
    "bddx['15AJ'] = bddx[['15A', '15J']].agg(' '.join, axis=1)\n",
    "\n",
    "bddx = bddx.drop(['Index', '1A', '1J', '2A', '2J', '3A', '3J', '4A', '4J', '5A', '5J', '6A', '6J', '7A', '7J', '8A', '8J', '9A', '9J', '10A', '10J', '11A', '11J', '12A', '12J', '13A', '13J', '14A', '14J', '15A', '15J', ], axis=1)\n",
    "bddx = bddx.drop(bddx.index[0])\n",
    "\n",
    "sdlList = []\n",
    "for index, row in bddx.iterrows():\n",
    "    sdlList.append(row.astype(str))\n",
    "    \n",
    "sdlStatements = []\n",
    "for i in range(len(sdlList)):\n",
    "    sdlStatements.append({'1': sdlList[i]['1AJ'], '2': sdlList[i]['2AJ'], '3': sdlList[i]['3AJ'], '4': sdlList[i]['4AJ'], \n",
    "                        '5': sdlList[i]['5AJ'], '6': sdlList[i]['6AJ'], '7': sdlList[i]['7AJ'], '8': sdlList[i]['8AJ'], \n",
    "                        '9': sdlList[i]['9AJ'], '10': sdlList[i]['10AJ'], '11': sdlList[i]['11AJ'], '12': sdlList[i]['12AJ'], \n",
    "                        '13': sdlList[i]['13AJ'], '14': sdlList[i]['14AJ'], '15': sdlList[i]['15AJ']})\n",
    "\n",
    "phrase_lookup = []\n",
    "\n",
    "for i in range(len(sdlStatements)):\n",
    "    keysTemp = list(sdlStatements[i].keys())\n",
    "    for j in keysTemp:\n",
    "        if (sdlStatements[i][j]==\" \"):\n",
    "            continue\n",
    "        else:\n",
    "            phrase_lookup.append(sdlStatements[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "EyOFpsdW1m79",
    "outputId": "d8a2b58e-cd8b-4012-86f5-4a953d3d1edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (0.16.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (4.41.1)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434675 sha256=2363641e49faeaf025691d1075b39fc625b06de2996279cb7bbffcd047030ecf\n",
      "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed nltk-3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "mG81JF2dXoPu",
    "outputId": "540658af-ca55-4964-ec23-7b7f544450fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# TODO: This is where metric stuff will go\n",
    "\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import nltk.translate.meteor_score as meteor\n",
    "import nltk\n",
    "from math import ceil\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def read_embeddings(embeddings_path):\n",
    "    \"\"\"Arguments:\n",
    "        - embeddings_path: path to the embeddings\n",
    "    \"\"\"\n",
    "    with open(embeddings_path, 'r') as in_stream:\n",
    "        embeddings = []\n",
    "        for line in in_stream:\n",
    "            line = '['+line.replace(' ',',')+']'\n",
    "            embeddings.append(eval(line))\n",
    "        print(\"Embeddings loaded\")\n",
    "        return embeddings\n",
    "    return []\n",
    "\n",
    "def load_from_file(filename,page_size=1000):\n",
    "    path = \"/Volumes/External HD/Deep Learning\"\n",
    "\n",
    "    with open(\"%s/%s_%i.pkl\"%(path,filename,0),\"rb\") as _init_in:\n",
    "        init_matrix = pkl.load(_init_in)\n",
    "\n",
    "    page_size = init_matrix.shape[0]\n",
    "    matrix_size = init_matrix.shape[1]\n",
    "\n",
    "    output = np.zeros((matrix_size,matrix_size))\n",
    "    output[0:page_size] = init_matrix\n",
    "\n",
    "    for i in range(1,ceil(matrix_size/page_size)):\n",
    "        with open(\"%s/%s_%i.pkl\"%(path,filename,i),\"rb\") as _init_in:\n",
    "            init_matrix = pkl.load(_init_in)\n",
    "        output[i*page_size:(i+1)*page_size] = init_matrix\n",
    "    \n",
    "    print(\"File loaded\")\n",
    "    return output\n",
    "\n",
    "def onehot_metric(a,b):\n",
    "    global sdl_matrix\n",
    "    if not 'sdl_matrix' in globals():\n",
    "        sdl_matrix = load_from_file(\"sdl_dist\")\n",
    "    return sdl_matrix[a][b]\n",
    "    \n",
    "def condensed_metric(a,b):\n",
    "    global sdl_matrix\n",
    "    if not 'sdl_matrix' in globals():\n",
    "        sdl_matrix = load_from_file(\"condensed_weighted_dist\")\n",
    "    return sdl_matrix[a][b]\n",
    "\n",
    "def sent2vec_metric(a,b): # a and b should be indices taken from similarity_dataset\n",
    "    global embeddings\n",
    "    if not 'embeddings' in globals():\n",
    "        embeddings = np.array(read_embeddings(\"../bddx_embeddings.txt\"))\n",
    "    return np.linalg.norm(embeddings[a]-embeddings[b]) # Euclidean Distance\n",
    "\n",
    "def bleu4_metric(a, b):\n",
    "    reference = [phrase_lookup[a].split()]\n",
    "    candidate = phrase_lookup[b].split()\n",
    "    smoothie = SmoothingFunction().method5\n",
    "    return 1.0-(bleu.sentence_bleu(reference, candidate, smoothing_function=smoothie, weights=[.25, .25, .25, .25]))\n",
    "\n",
    "def bleu1_metric(a, b):\n",
    "    reference = [phrase_lookup[a].split()]\n",
    "    candidate = phrase_lookup[b].split()\n",
    "    smoothie = SmoothingFunction().method5\n",
    "    return 1.0-(bleu.sentence_bleu(reference, candidate, smoothing_function=smoothie, weights=[1, 0, 0, 0]))\n",
    "\n",
    "def bleu_metric(a, b):\n",
    "    reference = [phrase_lookup[a].split()]\n",
    "    candidate = phrase_lookup[b].split()\n",
    "    smoothie = SmoothingFunction().method5\n",
    "    return 1.0-(bleu.sentence_bleu(reference, candidate, smoothing_function=smoothie, weights=[0.5, 0.5, 0, 0]))\n",
    "\n",
    "def meteor_metric(a, b):\n",
    "    return 1.0-meteor.single_meteor_score(phrase_lookup[a], phrase_lookup[b], alpha=.9, beta=5, gamma=0.9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0h0Mg23Ix8VH"
   },
   "source": [
    "Quick experiment i ran: i computed the metrics for an example in the similarity dataset, and got rankings as returned by a human (me), and then from the metrics. I did this to test out the kendall tau ranking system which can be found below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "wNSSj628Ygk_",
    "outputId": "ddda063b-fe3e-4b77-c82d-b6cc93a24039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base:  The car continues to drive forward. The car drives forward because there are no other cars or lights in its path.\n",
      "The car drives forward because there are no cars in front of it.\n",
      "The car drives forward because there are no nearby cars in its lane.\n",
      "The car is driving quickly down the dark road because there is no traffic and all the lights are green.\n",
      "The car speeds down the road because the lights are green and yellow.\n",
      "The car is driving in the middle lane because all the lights are green.\n",
      "The car drives forward because there are no other cars on the road.\n"
     ]
    }
   ],
   "source": [
    "base = phrase_lookup[1322]\n",
    "ref1 = phrase_lookup[8027]\n",
    "ref2 = phrase_lookup[6459]\n",
    "ref3 = phrase_lookup[9951]\n",
    "ref4 = phrase_lookup[16541]\n",
    "ref5 = phrase_lookup[21116]\n",
    "ref6 = phrase_lookup[4340]\n",
    "print('base: ', base)\n",
    "print(ref1)\n",
    "print(ref2)\n",
    "print(ref3)\n",
    "print(ref4)\n",
    "print(ref5)\n",
    "print(ref6)\n",
    "ranking = [5, 0, 1, 4, 3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "sqK86w0tkeOV",
    "outputId": "a78d0be6-a08d-4c75-8360-045601d27ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU1_METRIC\n",
      "0.5923977076871624\n",
      "0.5623736523267994\n",
      "0.605100192647316\n",
      "0.39102342897224723\n",
      "0.33372892304403495\n",
      "0.572406478252184\n",
      "BLEU_METRIC\n",
      "0.5240501824891005\n",
      "0.46880390171028075\n",
      "0.5041591703440566\n",
      "0.23884652750815316\n",
      "0.20526798546239827\n",
      "0.358219625867592\n",
      "BLEU4_METRIC\n",
      "0.4562821362623415\n",
      "0.37296378222362025\n",
      "0.3908389419610675\n",
      "0.0818909796416763\n",
      "0.07062264282595435\n",
      "0.12431709489727294\n",
      "METEOR_METRIC\n",
      "0.5438149750361964\n",
      "0.49396683168316824\n",
      "0.5438149750361964\n",
      "0.04433497536945812\n",
      "0.029702970297029695\n",
      "0.22419090909090905\n"
     ]
    }
   ],
   "source": [
    "# Groundtruth: base: \n",
    "print('BLEU1_METRIC')\n",
    "print(bleu1_metric(1322, 4340))\n",
    "print(bleu1_metric(1322, 8027))\n",
    "print(bleu1_metric(1322, 6459))\n",
    "print(bleu1_metric(1322, 21116))\n",
    "print(bleu1_metric(1322, 16541))\n",
    "print(bleu1_metric(1322, 9951))\n",
    "bleu1_ranking = [1, 5, 2, 0, 4, 3]\n",
    "# 6459 = 1, 4340 = 5, 8027 = 0, 9951 = 2, 2116 = 4, 16541 = 3\n",
    "\n",
    "print('BLEU_METRIC')\n",
    "print(bleu_metric(1322, 4340))\n",
    "print(bleu_metric(1322, 8027))\n",
    "print(bleu_metric(1322, 6459))\n",
    "print(bleu_metric(1322, 21116))\n",
    "print(bleu_metric(1322, 16541))\n",
    "print(bleu_metric(1322, 9951))\n",
    "bleu_ranking = [5, 1, 0, 2, 4, 3] \n",
    "\n",
    "print('BLEU4_METRIC')\n",
    "print(bleu4_metric(1322, 4340))\n",
    "print(bleu4_metric(1322, 8027))\n",
    "print(bleu4_metric(1322, 6459))\n",
    "print(bleu4_metric(1322, 21116))\n",
    "print(bleu4_metric(1322, 16541))\n",
    "print(bleu4_metric(1322, 9951))\n",
    "bleu4_ranking = [5, 1, 0, 2, 4, 3]\n",
    "\n",
    "print('METEOR_METRIC')\n",
    "print(meteor_metric(1322, 4340))\n",
    "print(meteor_metric(1322, 8027))\n",
    "print(meteor_metric(1322, 6459))\n",
    "print(meteor_metric(1322, 21116))\n",
    "print(meteor_metric(1322, 16541))\n",
    "print(meteor_metric(1322, 9951))\n",
    "meteor_ranking = [5, 1, 0, 2, 4, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wktXdvf_g3K5"
   },
   "outputs": [],
   "source": [
    "base_ranking = [5, 0, 1, 4, 3, 2]\n",
    "bleu1_ranking = [1, 5, 2, 0, 4, 3]\n",
    "bleu_ranking = [5, 1, 0, 2, 4, 3] \n",
    "bleu4_ranking = [5, 1, 0, 2, 4, 3]\n",
    "meteor_ranking = [5, 1, 0, 2, 4, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author's Note:\n",
    "How to change this code:\n",
    "  Simply change \"metric = sent2vec_metric\" to a different metric function. The metric function\n",
    "  should take as input two indices taken from the similarity_dataset.\n",
    "How to read metric_output:\n",
    "  For a given base index (e.g., 15096), it shows which ranking each video has under the metric\n",
    "  e.g., [1,0,2,5,3,4] mean that the second video (index 23555) is the most similar, and the\n",
    "  fourth video (index 17262) is the least similar.\n",
    "\"\"\"\n",
    "\n",
    "metric_output = {}\n",
    "metric = bleu1_metric # This should be a function that takes two indices as inputs\n",
    "\n",
    "for base in similarity_dataset:\n",
    "    metric_output[base] = [0]*len(similarity_dataset[base])\n",
    "    output = []\n",
    "    for comparison in similarity_dataset[base]:\n",
    "        output.append( metric(base,comparison) )\n",
    "    order = np.argsort(output)\n",
    "    for i,sort_order in enumerate(order):\n",
    "        metric_output[base][sort_order] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset file reads to run a new metric\n",
    "sdl_matrix = None\n",
    "del sdl_matrix\n",
    "embeddings = None\n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brk = 5\n",
    "for key in metric_output:\n",
    "    print(key)\n",
    "    print(metric_output[key])\n",
    "    print(similarity_dataset[key])\n",
    "    brk-=1\n",
    "    if brk <= 0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "masterfile = \"master_ranks.pkl\"\n",
    "\n",
    "if os.path.isfile(masterfile):\n",
    "    with open(masterfile,\"rb\") as _in:\n",
    "        d = pkl.load(_in)\n",
    "else:\n",
    "    d = {}\n",
    "\n",
    "abort = False\n",
    "if metric_name in d:\n",
    "    print(\"Metric name already exists!, Type 'y' to overwrite:\")\n",
    "    char = input(\"> \")\n",
    "    abort = True\n",
    "    if char == 'y':\n",
    "        abort = False\n",
    "        \n",
    "if not abort:\n",
    "    d[metric_name] = metric_output\n",
    "    with open(\"./master_ranks.pkl\",\"wb\") as _out:\n",
    "        pkl.dump(d,_out)\n",
    "        \n",
    "print(len(d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvGMWohz_oQI"
   },
   "source": [
    "# RBO (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "s5sKXp0N_q3E",
    "outputId": "9f7db2c5-d71e-4a95-9b6e-2777e8b865c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install -e git+https://github.com/changyaochen/rbo.git@master#egg=rbo\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "%pip install -e git+https://github.com/changyaochen/rbo.git@master#egg=rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_4NCA_fE_thJ",
    "outputId": "d512f044-0ecc-4c3d-89a5-a2178a352da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/src/rbo/rbo\n"
     ]
    }
   ],
   "source": [
    "%cd src/rbo/rbo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "wzVw53JGBdrn",
    "outputId": "9b18a1f2-b200-4d94-c883-93a1ea7962a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22599999999999995\n",
      "0.049008000000000045\n"
     ]
    }
   ],
   "source": [
    "import rbo\n",
    "'''\n",
    "Note there the choice of p is of great importance, since it essentically\n",
    "control the 'top-weightness'. Simply put, to an extreme, a small p value\n",
    "will only consider first few items, whereas a larger p value will\n",
    "consider more itmes. See Eq. (21) for quantitative measure.\n",
    "'''\n",
    "S = [1, 2, 3]\n",
    "T = [1, 3, 2]\n",
    "# Ranking Similarity function assumes p = 1.0 which is not what we want since we want to weight the top elements, whereas 1.0 just gives an intersection/overlap \n",
    "x = rbo.RankingSimilarity(S, T).rbo(p=.90)\n",
    "y = rbo.RankingSimilarity(S, T).rbo(p=.98)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rbo\n",
    "\n",
    "with open(\"groundtruth.pkl\",\"rb\") as _in:\n",
    "    gt = pkl.load(_in)\n",
    "\n",
    "with open(\"master_ranks.pkl\",\"rb\") as _in:\n",
    "    metric_ranks = pkl.load(_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'onehot': 0.232921\n",
      "'condensed': 0.243919\n",
      "'sent2vec': 0.270561\n",
      "'bleu1': 0.261855\n",
      "'bleu4': 0.261433\n",
      "'meteor': 0.259561\n"
     ]
    }
   ],
   "source": [
    "def score(l1, l2, p=0.98):\n",
    "    return rbo.RankingSimilarity(l1,l2).rbo(p=p)\n",
    "\n",
    "metric_scores = {}\n",
    "\n",
    "for metric in metric_ranks:\n",
    "    metric_scores[metric] = 0.0\n",
    "    for sample in metric_ranks[metric]:\n",
    "        metric_scores[metric]+=score(gt[sample],metric_ranks[metric][sample],p=0.9)\n",
    "    metric_scores[metric]/=len(metric_ranks[metric])\n",
    "\n",
    "for metric in metric_scores:\n",
    "    print(\"'%s': %f\"%(metric,metric_scores[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'onehot': (0.14798699999999998, 13187), 'condensed': (0.14798699999999998, 2299), 'sent2vec': (0.14798699999999998, 13651), 'bleu1': (0.14798699999999998, 25420), 'bleu4': (0.14798699999999998, 25420), 'meteor': (0.14798699999999998, 24201)}\n"
     ]
    }
   ],
   "source": [
    "metric_mins = {}\n",
    "\n",
    "for metric in metric_ranks:\n",
    "    for sample in metric_ranks[metric]:\n",
    "        scr = score(gt[sample],metric_ranks[metric][sample],p=0.9)\n",
    "        if not metric in metric_mins:\n",
    "            metric_mins[metric] = (scr,sample)\n",
    "        elif scr < metric_mins[metric][0]:\n",
    "            metric_mins[metric] = (scr,sample)\n",
    "\n",
    "print(metric_mins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 5, 4, 2, 1]\n",
      "[0, 3, 5, 1, 2, 4]\n",
      "[4, 2, 1, 3, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "sanity = 25420\n",
    "\n",
    "print(gt[sanity])\n",
    "print(metric_ranks['onehot'][sanity])\n",
    "print(metric_ranks['bleu4'][sanity])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwscxIq1yUCv"
   },
   "source": [
    "# RBO (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "3aFrTu4PhCrk",
    "outputId": "2ee15b70-1773-49b1-ec43-6ca3325cbf5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n",
      "0.99631052736\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def score(l1, l2, p = 0.98):\n",
    "    \"\"\"\n",
    "        Calculates Ranked Biased Overlap (RBO) score. \n",
    "        l1 -- Ranked List 1\n",
    "        l2 -- Ranked List 2\n",
    "    \"\"\"\n",
    "    if l1 == None: l1 = []\n",
    "    if l2 == None: l2 = []\n",
    "    \n",
    "    sl,ll = sorted([(len(l1), l1),(len(l2),l2)])\n",
    "    s, S = sl\n",
    "    l, L = ll\n",
    "    if s == 0: return 0\n",
    "\n",
    "    # Calculate the overlaps at ranks 1 through l \n",
    "    # (the longer of the two lists)\n",
    "    ss = set([]) # contains elements from the smaller list till depth i\n",
    "    ls = set([]) # contains elements from the longer list till depth i\n",
    "    x_d = {0: 0}\n",
    "    sum1 = 0.0\n",
    "    for i in range(l):\n",
    "        x = L[i]\n",
    "        y = S[i] if i < s else None\n",
    "        d = i + 1\n",
    "        \n",
    "        # if two elements are same then \n",
    "        # we don't need to add to either of the set\n",
    "        if x == y: \n",
    "            x_d[d] = x_d[d-1] + 1.0\n",
    "        # else add items to respective list\n",
    "        # and calculate overlap\n",
    "        else: \n",
    "            ls.add(x) \n",
    "            if y != None: ss.add(y)\n",
    "            x_d[d] = x_d[d-1] + (1.0 if x in ss else 0.0) + (1.0 if y in ls else 0.0)     \n",
    "        #calculate average overlap\n",
    "        sum1 += x_d[d]/d * pow(p, d)\n",
    "        \n",
    "    sum2 = 0.0\n",
    "    for i in range(l-s):\n",
    "        d = s+i+1\n",
    "        sum2 += x_d[d]*(d-s)/(d*s)*pow(p,d)\n",
    "\n",
    "    sum3 = ((x_d[l]-x_d[s])/l+x_d[s]/s)*pow(p,l)\n",
    "\n",
    "    # Equation 32\n",
    "    rbo_ext = (1-p)/p*(sum1+sum2)+sum3\n",
    "    return rbo_ext\n",
    "    \n",
    "    \n",
    "\n",
    "list1 = ['0','1','2','3','4','5']\n",
    "list2 = ['1','0','2','3','4','5']\n",
    "print (score(list1,list2, p = 0.98))\n",
    "\n",
    "list1 = ['1','0','2','3','4','5']\n",
    "list2 = ['1','0','2','3','5','4']\n",
    "print (score(list1, list2, p = 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "6oi-uBFAhpFK",
    "outputId": "645207e9-9254-40d3-fdbc-5b4496ed153b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM BLEU:  0.98180456736\n",
      "BLEU1:  0.9554019006933333\n",
      "BLEU4:  0.98180456736\n",
      "METEOR:  0.98180456736\n"
     ]
    }
   ],
   "source": [
    "print(\"CUSTOM BLEU: \", score(base_ranking, bleu_ranking))\n",
    "print(\"BLEU1: \", score(base_ranking, bleu1_ranking))\n",
    "print(\"BLEU4: \", score(base_ranking, bleu4_ranking))\n",
    "print(\"METEOR: \", score(base_ranking, meteor_ranking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bO1EVkL0XoP4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ranking.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
