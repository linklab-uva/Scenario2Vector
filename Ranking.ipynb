{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Ranking.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOejWJD-XoPl"
      },
      "source": [
        "import pickle as pkl\n",
        "import numpy as np\n",
        "\n",
        "path = \".\"\n",
        "filename = \"crowd_dict.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TirgjX2zXoPp"
      },
      "source": [
        "with open(\"%s/%s\"%(path,filename),\"rb\") as _in:\n",
        "    similarity_dataset = pkl.load(_in)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsubJI7vXoPs"
      },
      "source": [
        "\"\"\"\n",
        "Author's Note:\n",
        "This code aligns the textual description with the video clips. In order to get the textual\n",
        "  description corresponding to key #5 in the similarity dataset, simply call phrase_lookup[5]\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd # Pandas library enables data manipulation\n",
        "data_url = \"revisedBDDX.csv\"\n",
        "def load_bddx_data(csv_name):\n",
        "    column_names = ['Index', 'InputVideo', '1S', '1E', '1A', '1J', '2S', '2E', '2A', '2J', '3S', '3E', '3A', '3J',\n",
        "                    '4S', '4E', '4A', '4J','5S', '5E', '5A', '5J','6S', '6E', '6A', '6J','7S', '7E', '7A', '7J',\n",
        "                    '8S', '8E', '8A', '8J','9S', '9E', '9A', '9J','10S', '10E', '10A', '10J','11S', '11E', '11A', '11J',\n",
        "                    '12S', '12E', '12A', '12J','13S', '13E', '13A', '13J','14S', '14E', '14A', '14J','15S', '15E', '15A', '15J']\n",
        "    \n",
        "    return pd.read_csv(csv_name, names=column_names)\n",
        "\n",
        "bddx = load_bddx_data(data_url)\n",
        "bddx = bddx.drop(['1S', '1E','2S', '2E','3S', '3E','4S', '4E','5S', '5E','6S', '6E','7S', '7E','8S', '8E','9S', '9E','10S', '10E','11S', '11E','12S', '12E','13S', '13E','14S', '14E','15S', '15E', ], axis=1)\n",
        "bddx = bddx.fillna(\"\")\n",
        "\n",
        "bddx['1AJ'] = bddx[['1A', '1J']].agg(' '.join, axis=1)\n",
        "bddx['2AJ'] = bddx[['2A', '2J']].agg(' '.join, axis=1)\n",
        "bddx['3AJ'] = bddx[['3A', '3J']].agg(' '.join, axis=1)\n",
        "bddx['4AJ'] = bddx[['4A', '4J']].agg(' '.join, axis=1)\n",
        "bddx['5AJ'] = bddx[['5A', '5J']].agg(' '.join, axis=1)\n",
        "bddx['6AJ'] = bddx[['6A', '6J']].agg(' '.join, axis=1)\n",
        "bddx['7AJ'] = bddx[['7A', '7J']].agg(' '.join, axis=1)\n",
        "bddx['8AJ'] = bddx[['8A', '8J']].agg(' '.join, axis=1)\n",
        "bddx['9AJ'] = bddx[['9A', '9J']].agg(' '.join, axis=1)\n",
        "bddx['10AJ'] = bddx[['10A', '10J']].agg(' '.join, axis=1)\n",
        "bddx['11AJ'] = bddx[['11A', '11J']].agg(' '.join, axis=1)\n",
        "bddx['12AJ'] = bddx[['12A', '12J']].agg(' '.join, axis=1)\n",
        "bddx['13AJ'] = bddx[['13A', '13J']].agg(' '.join, axis=1)\n",
        "bddx['14AJ'] = bddx[['14A', '14J']].agg(' '.join, axis=1)\n",
        "bddx['15AJ'] = bddx[['15A', '15J']].agg(' '.join, axis=1)\n",
        "\n",
        "bddx = bddx.drop(['Index', '1A', '1J', '2A', '2J', '3A', '3J', '4A', '4J', '5A', '5J', '6A', '6J', '7A', '7J', '8A', '8J', '9A', '9J', '10A', '10J', '11A', '11J', '12A', '12J', '13A', '13J', '14A', '14J', '15A', '15J', ], axis=1)\n",
        "bddx = bddx.drop(bddx.index[0])\n",
        "\n",
        "sdlList = []\n",
        "for index, row in bddx.iterrows():\n",
        "    sdlList.append(row.astype(str))\n",
        "    \n",
        "sdlStatements = []\n",
        "for i in range(len(sdlList)):\n",
        "    sdlStatements.append({'1': sdlList[i]['1AJ'], '2': sdlList[i]['2AJ'], '3': sdlList[i]['3AJ'], '4': sdlList[i]['4AJ'], \n",
        "                        '5': sdlList[i]['5AJ'], '6': sdlList[i]['6AJ'], '7': sdlList[i]['7AJ'], '8': sdlList[i]['8AJ'], \n",
        "                        '9': sdlList[i]['9AJ'], '10': sdlList[i]['10AJ'], '11': sdlList[i]['11AJ'], '12': sdlList[i]['12AJ'], \n",
        "                        '13': sdlList[i]['13AJ'], '14': sdlList[i]['14AJ'], '15': sdlList[i]['15AJ']})\n",
        "\n",
        "phrase_lookup = []\n",
        "\n",
        "for i in range(len(sdlStatements)):\n",
        "    keysTemp = list(sdlStatements[i].keys())\n",
        "    for j in keysTemp:\n",
        "        if (sdlStatements[i][j]==\" \"):\n",
        "            continue\n",
        "        else:\n",
        "            phrase_lookup.append(sdlStatements[i][j])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyOFpsdW1m79",
        "outputId": "d8a2b58e-cd8b-4012-86f5-4a953d3d1edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!pip install nltk==3.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (0.16.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434675 sha256=2363641e49faeaf025691d1075b39fc625b06de2996279cb7bbffcd047030ecf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG81JF2dXoPu",
        "outputId": "540658af-ca55-4964-ec23-7b7f544450fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# TODO: This is where metric stuff will go\n",
        "\n",
        "import nltk.translate.bleu_score as bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "import nltk.translate.meteor_score as meteor\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def read_embeddings(embeddings_path):\n",
        "    \"\"\"Arguments:\n",
        "        - embeddings_path: path to the embeddings\n",
        "    \"\"\"\n",
        "    with open(embeddings_path, 'r') as in_stream:\n",
        "        embeddings = []\n",
        "        for line in in_stream:\n",
        "            line = '['+line.replace(' ',',')+']'\n",
        "            embeddings.append(eval(line))\n",
        "        return embeddings\n",
        "    return []\n",
        "\n",
        "#embeddings = np.array(read_embeddings(\"./bddx_embeddings.txt\"))\n",
        "\n",
        "def sent2vec_metric(a,b): # a and b should be indices taken from similarity_dataset\n",
        "    return np.linalg.norm(embeddings[a]-embeddings[b]) # Euclidean Distance\n",
        "\n",
        "\n",
        "def bleu4_metric(a, b):\n",
        "    reference = [phrase_lookup[a].split()]\n",
        "    candidate = phrase_lookup[b].split()\n",
        "    smoothie = SmoothingFunction().method5\n",
        "    return (bleu.sentence_bleu(reference, candidate, smoothing_function=smoothie, weights=[.25, .25, .25, .25]))\n",
        "\n",
        "def bleu1_metric(a, b):\n",
        "    reference = [phrase_lookup[a].split()]\n",
        "    candidate = phrase_lookup[b].split()\n",
        "    smoothie = SmoothingFunction().method5\n",
        "    return (bleu.sentence_bleu(reference, candidate, smoothing_function=smoothie, weights=[1, 0, 0, 0]))\n",
        "\n",
        "def bleu_metric(a, b):\n",
        "    reference = [phrase_lookup[a].split()]\n",
        "    candidate = phrase_lookup[b].split()\n",
        "    smoothie = SmoothingFunction().method5\n",
        "    return (bleu.sentence_bleu(reference, candidate, smoothing_function=smoothie, weights=[0.5, 0.5, 0, 0]))\n",
        "\n",
        "def meteor_metric(a, b):\n",
        "    return meteor.single_meteor_score(phrase_lookup[a], phrase_lookup[b], alpha=.9, beta=5, gamma=0.9)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h0Mg23Ix8VH"
      },
      "source": [
        "Quick experiment i ran: i computed the metrics for an example in the similarity dataset, and got rankings as returned by a human (me), and then from the metrics. I did this to test out the kendall tau ranking system which can be found below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNSSj628Ygk_",
        "outputId": "ddda063b-fe3e-4b77-c82d-b6cc93a24039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "base = phrase_lookup[1322]\n",
        "ref1 = phrase_lookup[8027]\n",
        "ref2 = phrase_lookup[6459]\n",
        "ref3 = phrase_lookup[9951]\n",
        "ref4 = phrase_lookup[16541]\n",
        "ref5 = phrase_lookup[21116]\n",
        "ref6 = phrase_lookup[4340]\n",
        "print('base: ', base)\n",
        "print(ref1)\n",
        "print(ref2)\n",
        "print(ref3)\n",
        "print(ref4)\n",
        "print(ref5)\n",
        "print(ref6)\n",
        "ranking = [5, 0, 1, 4, 3, 2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base:  The car continues to drive forward. The car drives forward because there are no other cars or lights in its path.\n",
            "The car drives forward because there are no cars in front of it.\n",
            "The car drives forward because there are no nearby cars in its lane.\n",
            "The car is driving quickly down the dark road because there is no traffic and all the lights are green.\n",
            "The car speeds down the road because the lights are green and yellow.\n",
            "The car is driving in the middle lane because all the lights are green.\n",
            "The car drives forward because there are no other cars on the road.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqK86w0tkeOV",
        "outputId": "a78d0be6-a08d-4c75-8360-045601d27ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# Groundtruth: base: \n",
        "print('BLEU1_METRIC')\n",
        "print(bleu1_metric(1322, 4340))\n",
        "print(bleu1_metric(1322, 8027))\n",
        "print(bleu1_metric(1322, 6459))\n",
        "print(bleu1_metric(1322, 21116))\n",
        "print(bleu1_metric(1322, 16541))\n",
        "print(bleu1_metric(1322, 9951))\n",
        "bleu1_ranking = [1, 5, 2, 0, 4, 3]\n",
        "# 6459 = 1, 4340 = 5, 8027 = 0, 9951 = 2, 2116 = 4, 16541 = 3\n",
        "\n",
        "print('BLEU_METRIC')\n",
        "print(bleu_metric(1322, 4340))\n",
        "print(bleu_metric(1322, 8027))\n",
        "print(bleu_metric(1322, 6459))\n",
        "print(bleu_metric(1322, 21116))\n",
        "print(bleu_metric(1322, 16541))\n",
        "print(bleu_metric(1322, 9951))\n",
        "bleu_ranking = [5, 1, 0, 2, 4, 3] \n",
        "\n",
        "print('BLEU4_METRIC')\n",
        "print(bleu4_metric(1322, 4340))\n",
        "print(bleu4_metric(1322, 8027))\n",
        "print(bleu4_metric(1322, 6459))\n",
        "print(bleu4_metric(1322, 21116))\n",
        "print(bleu4_metric(1322, 16541))\n",
        "print(bleu4_metric(1322, 9951))\n",
        "bleu4_ranking = [5, 1, 0, 2, 4, 3]\n",
        "\n",
        "print('METEOR_METRIC')\n",
        "print(meteor_metric(1322, 4340))\n",
        "print(meteor_metric(1322, 8027))\n",
        "print(meteor_metric(1322, 6459))\n",
        "print(meteor_metric(1322, 21116))\n",
        "print(meteor_metric(1322, 16541))\n",
        "print(meteor_metric(1322, 9951))\n",
        "meteor_ranking = [5, 1, 0, 2, 4, 3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU1_METRIC\n",
            "0.5923977076871624\n",
            "0.5623736523267994\n",
            "0.605100192647316\n",
            "0.39102342897224723\n",
            "0.33372892304403495\n",
            "0.572406478252184\n",
            "BLEU_METRIC\n",
            "0.5240501824891005\n",
            "0.46880390171028075\n",
            "0.5041591703440566\n",
            "0.23884652750815316\n",
            "0.20526798546239827\n",
            "0.358219625867592\n",
            "BLEU4_METRIC\n",
            "0.4562821362623415\n",
            "0.37296378222362025\n",
            "0.3908389419610675\n",
            "0.0818909796416763\n",
            "0.07062264282595435\n",
            "0.12431709489727294\n",
            "METEOR_METRIC\n",
            "0.5438149750361964\n",
            "0.49396683168316824\n",
            "0.5438149750361964\n",
            "0.04433497536945812\n",
            "0.029702970297029695\n",
            "0.22419090909090905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wktXdvf_g3K5"
      },
      "source": [
        "base_ranking = [5, 0, 1, 4, 3, 2]\n",
        "bleu1_ranking = [1, 5, 2, 0, 4, 3]\n",
        "bleu_ranking = [5, 1, 0, 2, 4, 3] \n",
        "bleu4_ranking = [5, 1, 0, 2, 4, 3]\n",
        "meteor_ranking = [5, 1, 0, 2, 4, 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwscxIq1yUCv"
      },
      "source": [
        "# Kendall Tau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aFrTu4PhCrk",
        "outputId": "2ee15b70-1773-49b1-ec43-6ca3325cbf5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "def score(l1, l2, p = 0.98):\n",
        "    \"\"\"\n",
        "        Calculates Ranked Biased Overlap (RBO) score. \n",
        "        l1 -- Ranked List 1\n",
        "        l2 -- Ranked List 2\n",
        "    \"\"\"\n",
        "    if l1 == None: l1 = []\n",
        "    if l2 == None: l2 = []\n",
        "    \n",
        "    sl,ll = sorted([(len(l1), l1),(len(l2),l2)])\n",
        "    s, S = sl\n",
        "    l, L = ll\n",
        "    if s == 0: return 0\n",
        "\n",
        "    # Calculate the overlaps at ranks 1 through l \n",
        "    # (the longer of the two lists)\n",
        "    ss = set([]) # contains elements from the smaller list till depth i\n",
        "    ls = set([]) # contains elements from the longer list till depth i\n",
        "    x_d = {0: 0}\n",
        "    sum1 = 0.0\n",
        "    for i in range(l):\n",
        "        x = L[i]\n",
        "        y = S[i] if i < s else None\n",
        "        d = i + 1\n",
        "        \n",
        "        # if two elements are same then \n",
        "        # we don't need to add to either of the set\n",
        "        if x == y: \n",
        "            x_d[d] = x_d[d-1] + 1.0\n",
        "        # else add items to respective list\n",
        "        # and calculate overlap\n",
        "        else: \n",
        "            ls.add(x) \n",
        "            if y != None: ss.add(y)\n",
        "            x_d[d] = x_d[d-1] + (1.0 if x in ss else 0.0) + (1.0 if y in ls else 0.0)     \n",
        "        #calculate average overlap\n",
        "        sum1 += x_d[d]/d * pow(p, d)\n",
        "        \n",
        "    sum2 = 0.0\n",
        "    for i in range(l-s):\n",
        "        d = s+i+1\n",
        "        sum2 += x_d[d]*(d-s)/(d*s)*pow(p,d)\n",
        "\n",
        "    sum3 = ((x_d[l]-x_d[s])/l+x_d[s]/s)*pow(p,l)\n",
        "\n",
        "    # Equation 32\n",
        "    rbo_ext = (1-p)/p*(sum1+sum2)+sum3\n",
        "    return rbo_ext\n",
        "    \n",
        "    \n",
        "\n",
        "list1 = ['0','1','2','3','4','5']\n",
        "list2 = ['1','0','2','3','4','5']\n",
        "print (score(list1,list2, p = 0.98))\n",
        "\n",
        "list1 = ['1','0','2','3','4','5']\n",
        "list2 = ['1','0','2','3','5','4']\n",
        "print (score(list1, list2, p = 0.98))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.98\n",
            "0.99631052736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oi-uBFAhpFK",
        "outputId": "645207e9-9254-40d3-fdbc-5b4496ed153b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"CUSTOM BLEU: \", score(base_ranking, bleu_ranking))\n",
        "print(\"BLEU1: \", score(base_ranking, bleu1_ranking))\n",
        "print(\"BLEU4: \", score(base_ranking, bleu4_ranking))\n",
        "print(\"METEOR: \", score(base_ranking, meteor_ranking))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUSTOM BLEU:  0.98180456736\n",
            "BLEU1:  0.9554019006933333\n",
            "BLEU4:  0.98180456736\n",
            "METEOR:  0.98180456736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQti3qViXoPx"
      },
      "source": [
        "\"\"\"\n",
        "Author's Note:\n",
        "How to change this code:\n",
        "  Simply change \"metric = sent2vec_metric\" to a different metric function. The metric function\n",
        "  should take as input two indices taken from the similarity_dataset.\n",
        "How to read metric_output:\n",
        "  For a given base index (e.g., 15096), it shows which ranking each video has under the metric\n",
        "  e.g., [1,0,2,5,3,4] mean that the second video (index 23555) is the most similar, and the\n",
        "  fourth video (index 17262) is the least similar.\n",
        "\"\"\"\n",
        "\n",
        "metric_output = {}\n",
        "metric = bleu1_metric # This should be a function that takes two indices as inputs\n",
        "\n",
        "for base in similarity_dataset:\n",
        "    metric_output[base] = [0]*len(similarity_dataset[base])\n",
        "    output = []\n",
        "    for comparison in similarity_dataset[base]:\n",
        "        output.append( metric(base,comparison) )\n",
        "    order = np.argsort(output)\n",
        "    for i,sort_order in enumerate(order):\n",
        "        metric_output[base][sort_order] = i\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAQ8AQCKXoP1",
        "outputId": "d5706274-8210-445c-dfc6-c233d5194d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "brk = 5\n",
        "for key in metric_output:\n",
        "    print(key)\n",
        "    print(metric_output[key])\n",
        "    print(similarity_dataset[key])\n",
        "    brk-=1\n",
        "    if brk <= 0:\n",
        "        break\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15096\n",
            "[3, 4, 1, 5, 2, 0]\n",
            "[14293, 23555, 2079, 24844, 17262, 15380]\n",
            "15985\n",
            "[1, 5, 3, 2, 0, 4]\n",
            "[19200, 15559, 2659, 11364, 898, 18227]\n",
            "15373\n",
            "[3, 4, 0, 2, 1, 5]\n",
            "[1399, 5208, 10498, 9388, 17117, 1904]\n",
            "7359\n",
            "[5, 3, 4, 2, 1, 0]\n",
            "[738, 14371, 5451, 24023, 26526, 13481]\n",
            "8812\n",
            "[2, 5, 0, 3, 1, 4]\n",
            "[18824, 13899, 24471, 2071, 2451, 8295]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO1EVkL0XoP4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}